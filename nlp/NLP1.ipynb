{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BIvhwE4zxX0s",
    "outputId": "ab964b49-89e7-45bb-e2da-5f9881bc37e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
      "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.2.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.1 MB 1.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: bokeh in /usr/local/lib/python3.7/dist-packages (2.3.3)\n",
      "Collecting bokeh\n",
      "  Downloading bokeh-2.4.3-py3-none-any.whl (18.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 18.5 MB 364 kB/s \n",
      "\u001b[?25hCollecting umap-learn\n",
      "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
      "\u001b[K     |████████████████████████████████| 88 kB 7.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.7.3)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.7/dist-packages (from bokeh) (3.13)\n",
      "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.7/dist-packages (from bokeh) (7.1.2)\n",
      "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.7/dist-packages (from bokeh) (21.3)\n",
      "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.7/dist-packages (from bokeh) (5.1.1)\n",
      "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.7/dist-packages (from bokeh) (2.11.3)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.7/dist-packages (from bokeh) (4.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.9->bokeh) (2.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=16.8->bokeh) (3.0.9)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.0.2)\n",
      "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (0.51.2)\n",
      "Collecting pynndescent>=0.5\n",
      "  Downloading pynndescent-0.5.7.tar.gz (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 54.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn) (0.34.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn) (57.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->umap-learn) (3.1.0)\n",
      "Building wheels for collected packages: umap-learn, pynndescent\n",
      "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82829 sha256=cd53623004fff2910df1325efdc2606c135229746c9532abb03f363c09256697\n",
      "  Stored in directory: /root/.cache/pip/wheels/b3/52/a5/1fd9e3e76a7ab34f134c07469cd6f16e27ef3a37aeff1fe821\n",
      "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pynndescent: filename=pynndescent-0.5.7-py3-none-any.whl size=54286 sha256=d84351a725eb1a11923b94bdc6a9c4b54e1821db3f34ce7f092e720509f9d2eb\n",
      "  Stored in directory: /root/.cache/pip/wheels/7f/2a/f8/7bd5dcec71bd5c669f6f574db3113513696b98f3f9b51f496c\n",
      "Successfully built umap-learn pynndescent\n",
      "Installing collected packages: pynndescent, umap-learn, gensim, bokeh\n",
      "  Attempting uninstall: gensim\n",
      "    Found existing installation: gensim 3.6.0\n",
      "    Uninstalling gensim-3.6.0:\n",
      "      Successfully uninstalled gensim-3.6.0\n",
      "  Attempting uninstall: bokeh\n",
      "    Found existing installation: bokeh 2.3.3\n",
      "    Uninstalling bokeh-2.3.3:\n",
      "      Successfully uninstalled bokeh-2.3.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "panel 0.12.1 requires bokeh<2.4.0,>=2.3.0, but you have bokeh 2.4.3 which is incompatible.\u001b[0m\n",
      "Successfully installed bokeh-2.4.3 gensim-4.2.0 pynndescent-0.5.7 umap-learn-0.5.3\n"
     ]
    }
   ],
   "source": [
    "#Установка нужных пакетов\n",
    "!pip install --upgrade nltk gensim bokeh umap-learn\n",
    "\n",
    "import itertools\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import umap\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hF9WPCtfxZR9",
    "outputId": "f487464b-7dc8-4ffa-817a-147b11f5f07b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-07-18 16:25:10--  https://www.dropbox.com/s/obaitrix9jyu84r/quora.txt?dl=1\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.64.18, 2620:100:6019:18::a27d:412\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.64.18|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: /s/dl/obaitrix9jyu84r/quora.txt [following]\n",
      "--2022-07-18 16:25:10--  https://www.dropbox.com/s/dl/obaitrix9jyu84r/quora.txt\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://uc9cf9a26d8ad345dff2f7bd0d42.dl.dropboxusercontent.com/cd/0/get/BpWBt2a5uXSYp3woTesOqsr4rTYN5r3f1g0NI8Rj9YzhVI-56aGKmMt4qlb17kj7RnGlu6Qs45TAivSTEaDbmWImOlb-HO2ILgkoPp8ojuDKCeUS4DYGBKwPpVNEm-jM2g9mUruq9WLOMGHZxrvPj-3g87F59dMtY5WAszSiyjSyAA/file?dl=1# [following]\n",
      "--2022-07-18 16:25:11--  https://uc9cf9a26d8ad345dff2f7bd0d42.dl.dropboxusercontent.com/cd/0/get/BpWBt2a5uXSYp3woTesOqsr4rTYN5r3f1g0NI8Rj9YzhVI-56aGKmMt4qlb17kj7RnGlu6Qs45TAivSTEaDbmWImOlb-HO2ILgkoPp8ojuDKCeUS4DYGBKwPpVNEm-jM2g9mUruq9WLOMGHZxrvPj-3g87F59dMtY5WAszSiyjSyAA/file?dl=1\n",
      "Resolving uc9cf9a26d8ad345dff2f7bd0d42.dl.dropboxusercontent.com (uc9cf9a26d8ad345dff2f7bd0d42.dl.dropboxusercontent.com)... 162.125.4.15, 2620:100:6019:15::a27d:40f\n",
      "Connecting to uc9cf9a26d8ad345dff2f7bd0d42.dl.dropboxusercontent.com (uc9cf9a26d8ad345dff2f7bd0d42.dl.dropboxusercontent.com)|162.125.4.15|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 33813903 (32M) [application/binary]\n",
      "Saving to: ‘./quora.txt’\n",
      "\n",
      "./quora.txt         100%[===================>]  32.25M  17.0MB/s    in 1.9s    \n",
      "\n",
      "2022-07-18 16:25:13 (17.0 MB/s) - ‘./quora.txt’ saved [33813903/33813903]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# выгружаем датасет:\n",
    "!wget https://www.dropbox.com/s/obaitrix9jyu84r/quora.txt?dl=1 -O ./quora.txt -nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "MaFpN9pvxtNg",
    "outputId": "2c6a9953-5d63-440a-cbaf-4d3ea8710dbd"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"What TV shows or books help you read people's body language?\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = list(open(\"./quora.txt\", encoding=\"utf-8\"))\n",
    "data[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HvXRbOKGx0l_",
    "outputId": "f0f5eead-f4ce-4bdd-e009-adc2d5c3d08b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Can', 'I', 'get', 'back', 'with', 'my', 'ex', 'even', 'though', 'she', 'is', 'pregnant', 'with', 'another', 'guy', \"'\", 's', 'baby', '?']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "print(tokenizer.tokenize(data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4APbZkpElkGk"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovkxi_QOySCl"
   },
   "source": [
    "#Задание 1: Перевести все слова в нижний регистр (NLTK) из data и добавьте как лист токенов в листе data_tok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wAJROQt21S9l",
    "outputId": "fbcf2d41-a235-44c4-a190-5316c0c2c809"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['can',\n",
       " 'i',\n",
       " 'get',\n",
       " 'back',\n",
       " 'with',\n",
       " 'my',\n",
       " 'ex',\n",
       " 'even',\n",
       " 'though',\n",
       " 'she',\n",
       " 'is',\n",
       " 'pregnant',\n",
       " 'with',\n",
       " 'another',\n",
       " 'guy',\n",
       " \"'\",\n",
       " 's',\n",
       " 'baby',\n",
       " '?']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tok[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "EK7uvHi6zeWY"
   },
   "outputs": [],
   "source": [
    "data_tok = []\n",
    "for i in range(len(data)): \n",
    "  data_tok.append(tokenizer.tokenize(data[i].lower()))\n",
    "\n",
    "#checking\n",
    "assert all(isinstance(row, (list, tuple)) for row in data_tok), \"please convert each line into a list of tokens (strings)\"\n",
    "assert all(all(isinstance(tok, str) for tok in row) for row in data_tok), \"please convert each line into a list of tokens (strings)\"\n",
    "is_latin = lambda tok: all('a' <= x.lower() <= 'z' for x in tok)\n",
    "assert all(map(lambda l: not is_latin(l) or l.islower(), map(' '.join, data_tok))), \"please make sure to lowercase the data\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dtKeoLCYzY4j"
   },
   "source": [
    "###Задание 2: Подсчитайте топ10 самых популярных лем в рамках data_tok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "a_BxzSv9yR0w",
    "outputId": "552e130c-f84d-47f5-dd3e-aac90d16f8d5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-c6ff7afc-120a-4dc6-9466-ce0ef7ba069f\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>?</th>\n",
       "      <td>552413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>252068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>what</th>\n",
       "      <td>214798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>185392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>155726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>149740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>141788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>139787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>how</th>\n",
       "      <td>135687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>112001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c6ff7afc-120a-4dc6-9466-ce0ef7ba069f')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-c6ff7afc-120a-4dc6-9466-ce0ef7ba069f button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-c6ff7afc-120a-4dc6-9466-ce0ef7ba069f');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "           0\n",
       "?     552413\n",
       "the   252068\n",
       "what  214798\n",
       "is    185392\n",
       "a     155726\n",
       "i     149740\n",
       "to    141788\n",
       "in    139787\n",
       "how   135687\n",
       "of    112001"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_words = {}\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for token in data_tok:\n",
    "  for word in token:\n",
    "    word_lemmatizing = lemmatizer.lemmatize(word)\n",
    "    if word in dict_words.keys(): \n",
    "      dict_words[word] += 1\n",
    "    else:\n",
    "      dict_words[word] = 1\n",
    "\n",
    "import pandas as pd\n",
    "df1 = pd.DataFrame.from_dict(dict_words, orient='index')\n",
    "df1\n",
    "df1.sort_values(0, ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MDgQ04r_5ofg",
    "outputId": "fdfd5bca-d316-4feb-a685-0bf8fc16cbb0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what',\n",
       " 'are',\n",
       " 'the',\n",
       " 'most',\n",
       " 'unintentionally',\n",
       " 'hilarious',\n",
       " 'movies',\n",
       " 'you',\n",
       " 'have',\n",
       " 'ever',\n",
       " 'seen',\n",
       " '?']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a1SM3sn1zf1b"
   },
   "source": [
    "###Задание 3: Подсчитайте количество разных слов до и после лемматизации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q88BIteDzpWR",
    "outputId": "9ac8ccf5-5077-4c02-9fb0-b9e195c4a86c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87819"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DrGlij0-S4Mp",
    "outputId": "556ae1bf-c74c-4273-95da-da42b0a203fa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "80303"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download( 'wordnet')\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "wnl  = WordNetLemmatizer()\n",
    "\n",
    "dict_words_lemm = {}\n",
    "\n",
    "for token in data_tok:\n",
    "  for word in token:\n",
    "    word = wnl.lemmatize(word)\n",
    "    if word in dict_words_lemm.keys(): \n",
    "      dict_words_lemm[word] += 1\n",
    "    else:\n",
    "      dict_words_lemm[word] = 1    \n",
    "      \n",
    "df = pd.DataFrame.from_dict(dict_words_lemm, orient='index')\n",
    "df.shape[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uxKa8yUUzqNN"
   },
   "source": [
    "###Задание 4: Подсчитайте количество разных слов до и после стемминга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x91DX51qzszR",
    "outputId": "68af23a1-8da1-42f0-ce35-0a1bfac72dc1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67203"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snow_stemmer = SnowballStemmer(language='english')\n",
    "dict_words_stem = {}\n",
    "\n",
    "for token in data_tok:\n",
    "  for word in token:\n",
    "    word1 = snow_stemmer.stem(word)\n",
    "    if word1 in dict_words_stem.keys(): \n",
    "      dict_words_stem[word1] += 1\n",
    "    else:\n",
    "      dict_words_stem[word1] = 1   \n",
    "df2 = pd.DataFrame.from_dict(dict_words_stem, orient='index')\n",
    "df2.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XXA7Fe_izuqh"
   },
   "source": [
    "###Задание 5: Подсчитайте количество разных слов\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BGgmHzUAzwqO",
    "outputId": "75737bf9-2456-47e1-fc0c-94f67860f618"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87819"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_words1 = {}\n",
    "for token in data_tok:\n",
    "  for word in token:\n",
    "    if word in dict_words1.keys(): \n",
    "      dict_words1[word] += 1\n",
    "    else:\n",
    "      dict_words1[word] = 1\n",
    "\n",
    "df3 = pd.DataFrame.from_dict(dict_words1, orient='index')\n",
    "df3.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "At9iloRCVShn"
   },
   "source": [
    "REGEXP\n",
    "\n",
    "https://www.programiz.com/python-programming/regex \n",
    "\n",
    "https://docs.python.org/3/howto/regex.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "521aLisyVUg_",
    "outputId": "bc023bbd-1a73-4899-e9e4-06a3b0d36c2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search successful.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pattern = 'a.*s'\n",
    "test_string = 'abyss'\n",
    "result = re.match(pattern, test_string)\n",
    "\n",
    "if result:\n",
    "  print(\"Search successful.\")\n",
    "else:\n",
    "  print(\"Search unsuccessful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dH7qx_irU4Y8"
   },
   "source": [
    "###Задание 6: \n",
    "https://www.hackerrank.com/challenges/matching-specific-string/problem?isFullScreen=true \n",
    "\n",
    "###Задание 7: \n",
    "https://www.hackerrank.com/challenges/matching-whitespace-non-whitespace-character/problem?isFullScreen=true\n",
    "\n",
    "###Задание 8: \n",
    "https://www.hackerrank.com/challenges/matching-start-end/problem?isFullScreen=true\n",
    "\n",
    "###Задание 9: \n",
    "https://www.hackerrank.com/challenges/matching-word-boundaries/problem?isFullScreen=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search successful.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "Test_String = 'The hackerrank team is on a mission to flatten the world by restructuring the DNA of every company on the planet. We rank programmers based on their coding skills, helping companies source great programmers and reduce the time to hire. As a result, we are revolutionizing the way companies discover and evaluate talented engineers. The hackerrank platform is the destination for the best engineers to hone their skills and companies to find top engineers.'\n",
    "Regex_Pattern = r'hackerrank'\n",
    "\n",
    "match = re.findall(Regex_Pattern, Test_String)\n",
    "\n",
    "if result:\n",
    "  print(\"Search successful.\")\n",
    "else:\n",
    "  print(\"Search unsuccessful.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    }
   ],
   "source": [
    "Regex_Pattern = r\"\\S\"\n",
    "\n",
    "import re\n",
    "\n",
    "print(str(bool(re.search(Regex_Pattern, '12 11 15'))).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    }
   ],
   "source": [
    "Regex_Pattern = r\"^\\d|\\S$\"\n",
    "import re\n",
    "\n",
    "print(str(bool(re.search(Regex_Pattern, '0qwer.'))).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    }
   ],
   "source": [
    "Regex_Pattern = r'\\b[aeiouAEIOU]'\n",
    "import re\n",
    "\n",
    "print(str(bool(re.search(Regex_Pattern, 'Found any match?'))).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    }
   ],
   "source": [
    "#ML1_1:\n",
    "#https://www.hackerrank.com/challenges/capturing-non-capturing-groups/problem?isFullScreen=true\n",
    "\n",
    "Regex_Pattern = r'[ok]{3,}'\n",
    "import re\n",
    "\n",
    "print(str(bool(re.search(Regex_Pattern, 'okokok! cya'))).lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Csv2YN2IRXJB"
   },
   "source": [
    "Bag Of Words (BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O8YWG3JhSFeZ",
    "outputId": "325b4fa5-7d56-4993-b4d0-1aae78bad94e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['welcome', 'to', 'great', 'learning', ',', 'now', 'start', 'learning']\n",
      "['learning', 'is', 'a', 'good', 'practice']\n",
      "['welcome', 'to', 'great', 'learning', ',', 'now', 'start', 'is', 'a', 'good', 'practice']\n",
      "['welcome', 'great', 'learning', 'now', 'start', 'good', 'practice']\n",
      "[1, 1, 2, 1, 1, 0, 0]\n",
      "[0, 0, 1, 0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "def vectorize(tokens):\n",
    "    ''' This function takes list of words in a sentence as input \n",
    "    and returns a vector of size of filtered_vocab.It puts 0 if the \n",
    "    word is not present in tokens and count of token if present.'''\n",
    "    vector=[]\n",
    "    for w in filtered_vocab:\n",
    "        vector.append(tokens.count(w))\n",
    "    return vector\n",
    "def unique(sequence):\n",
    "    '''This functions returns a list in which the order remains \n",
    "    same and no item repeats.Using the set() function does not \n",
    "    preserve the original ordering,so i didnt use that instead'''\n",
    "    seen = set()\n",
    "    return [x for x in sequence if not (x in seen or seen.add(x))]\n",
    "\n",
    "#create a list of stopwords.You can import stopwords from nltk too\n",
    "stopwords=[\"to\",\"is\",\"a\"]\n",
    "\n",
    "#list of special characters.You can use regular expressions too\n",
    "special_char=[\",\",\":\",\" \",\";\",\".\",\"?\"]\n",
    "\n",
    "#Write the sentences in the corpus,in our case, just two \n",
    "string1=\"Welcome to Great Learning , Now start learning\"\n",
    "string2=\"Learning is a good practice\"\n",
    "\n",
    "#convert them to lower case\n",
    "string1=string1.lower()\n",
    "string2=string2.lower()\n",
    "\n",
    "#split the sentences into tokens\n",
    "tokens1=string1.split()\n",
    "tokens2=string2.split()\n",
    "print(tokens1)\n",
    "print(tokens2)\n",
    "\n",
    "#create a vocabulary list\n",
    "vocab=unique(tokens1+tokens2)\n",
    "print(vocab)\n",
    "\n",
    "#filter the vocabulary list\n",
    "filtered_vocab=[]\n",
    "for w in vocab: \n",
    "    if w not in stopwords and w not in special_char: \n",
    "        filtered_vocab.append(w)\n",
    "print(filtered_vocab)\n",
    "\n",
    "#convert sentences into vectords\n",
    "vector1=vectorize(tokens1)\n",
    "print(vector1)\n",
    "vector2=vectorize(tokens2)\n",
    "print(vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MOZ1qx05Q46b"
   },
   "source": [
    "Задание 10: Реализовать Bag of words на data_tok (можно на NLTK, можно без)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tew2nQN4OCiW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Копия блокнота \"NLP1\"",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
